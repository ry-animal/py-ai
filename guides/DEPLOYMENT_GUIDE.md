# Enterprise Deployment Guide

Complete guide for deploying the Python AI Multi-Agent Platform using enterprise technologies.

## ğŸ¯ Quick Start Enterprise Deployment

### Prerequisites
- **Harness Account** with CI/CD pipelines
- **Google Cloud Project** or multi-cloud access
- **Terraform** >= 1.0 installed
- **Docker** and container registry access
- **Database Access**: MongoDB, PostgreSQL, Redis

### 1. Initial Setup

```bash
# Clone and setup
git clone <your-repo>
cd py-ai

# Install with enterprise dependencies
uv add --group enterprise

# Copy enterprise environment template
cp .env.enterprise.example .env
# Edit .env with your actual values
```

### 2. Configure Databases

Choose your vector database strategy:

**Option A: Vertex AI (Recommended for GCP)**
```bash
# Set in .env
VECTOR_DB_TYPE=vertex
GCP_PROJECT_ID=your-project-id
VERTEX_INDEX_ENDPOINT_ID=your-endpoint-id
```

**Option B: Snowflake Cortex**
```bash
# Set in .env
VECTOR_DB_TYPE=snowflake
SNOWFLAKE_ACCOUNT=your-account.snowflakecomputing.com
SNOWFLAKE_USER=your-username
SNOWFLAKE_PASSWORD=your-password
```

**Option C: CockroachDB (Bonus Points! ğŸ¯)**
```bash
# Set in .env
VECTOR_DB_TYPE=cockroach
COCKROACH_CONNECTION_STRING=postgresql://user:pass@host:26257/db
```

### 3. Deploy Infrastructure with Terraform

```bash
cd terraform/environments/staging

# Initialize Terraform
terraform init

# Copy variables template
cp terraform.tfvars.example terraform.tfvars
# Edit terraform.tfvars with your values

# Plan deployment
terraform plan

# Deploy infrastructure
terraform apply
```

### 4. Set Up Harness CI/CD Pipeline

1. **Import Pipeline**: Upload `.harness/pipelines/py-ai-pipeline.yaml`
2. **Configure Connectors**:
   - Docker registry connector
   - Kubernetes cluster connector
   - Cloud provider connector
3. **Set Pipeline Variables**:
   - `docker_registry`: Your container registry
   - `kubernetes_cluster`: Target K8s cluster
4. **Configure Secrets**:
   - API keys (Anthropic, OpenAI, Tavily)
   - Database connection strings
   - Slack webhook for notifications

### 5. Run First Deployment

```bash
# Trigger Harness pipeline manually or push to main branch
git push origin main

# Monitor pipeline execution in Harness UI
# Pipeline will:
# 1. Run all agent tests
# 2. Build and scan containers
# 3. Deploy to staging
# 4. Run smoke tests
# 5. Deploy to production (with approval)
```

## ğŸ—ï¸ Architecture Overview

The deployed system includes:

### Infrastructure Components
- **Google Cloud Run**: Scalable container hosting
- **PostgreSQL**: Metadata and session management  
- **MongoDB**: Document storage
- **Redis**: Caching and Celery message broker
- **Vector Database**: Choice of Vertex AI, Snowflake, or CockroachDB

### Multi-Agent System
- **Smart Orchestrator** (`/smart/chat`): Intelligent agent selection
- **LangGraph Agent** (`/agent/chat`): Complex workflows
- **Pydantic-AI Agent** (`/pydantic-agent/chat`): Type-safe operations
- **Hybrid Agent** (`/hybrid-agent/chat`): Best of both worlds

### Enterprise Features
- **CI/CD Pipeline**: Automated testing and deployment with Harness
- **Infrastructure as Code**: Terraform modules for reproducible deployments
- **Multi-Cloud Support**: Vector databases across cloud providers
- **Security Scanning**: Container and dependency vulnerability scanning
- **Monitoring**: OpenTelemetry traces and structured logging

## ğŸ“Š Verification

After deployment, verify all systems:

```bash
# Get deployment URLs from Terraform output
terraform output

# Test health endpoints
curl https://your-api-url/health
curl https://your-api-url/ready

# Test all agent architectures
curl "https://your-api-url/smart/chat?q=Hello"
curl "https://your-api-url/agent/chat?q=Hello"
curl "https://your-api-url/pydantic-agent/chat?q=Hello"
curl "https://your-api-url/hybrid-agent/chat?q=Hello"

# Check agent status
curl https://your-api-url/smart/status
curl https://your-api-url/pydantic-agent/status
curl https://your-api-url/hybrid-agent/status

# Test orchestrator intelligence
curl -X POST https://your-api-url/smart/analyze \
  -H "Content-Type: application/json" \
  -d '{"question": "Extract user data in JSON format"}'
```

## ğŸ”§ Configuration Management

### Environment-Specific Configurations

**Development**:
```bash
VECTOR_DB_TYPE=chroma  # Local development
MONGODB_URL=mongodb://localhost:27017/py_ai_dev
REDIS_URL=redis://localhost:6379/0
```

**Staging**:
```bash
VECTOR_DB_TYPE=vertex  # Cloud vector DB
MONGODB_URL=mongodb+srv://staging-cluster/py_ai_staging
REDIS_URL=redis://staging-redis:6379/0
```

**Production**:
```bash
VECTOR_DB_TYPE=vertex  # Production vector DB
MONGODB_URL=mongodb+srv://prod-cluster/py_ai_production
REDIS_URL=redis://prod-redis:6379/0
ENABLE_METRICS=true
OTEL_ENDPOINT=http://jaeger:14268/api/traces
```

### Feature Flags

Control enterprise features via environment variables:

```bash
# Multi-Agent Features
ENABLE_SMART_ORCHESTRATOR=true
ENABLE_HYBRID_AGENT=true
ENABLE_PYDANTIC_AGENT=true
ENABLE_LANGGRAPH_AGENT=true

# Enterprise Integrations
LIGHTLLM_ENABLED=true
ENABLE_PLUGINS=true
ENABLE_METRICS=true
```

## ğŸš€ Scaling Considerations

### Horizontal Scaling
- **API Instances**: Auto-scaling via Cloud Run (1-10 instances)
- **Worker Instances**: Celery workers scale independently (1-5 instances)
- **Database Connections**: Connection pooling for PostgreSQL and MongoDB

### Performance Optimization
- **Vector Search**: Optimized indexes for each database type
- **Caching**: Redis for session data and frequent queries
- **Agent Selection**: LightLLM for intelligent model routing

### Cost Optimization
- **Multi-Cloud**: Choose cost-effective vector database per region
- **Auto-Scaling**: Scale down during low traffic periods
- **Resource Limits**: Configured CPU/memory limits per service

## ğŸ” Security

### Network Security
- **VPC Networks**: Private networking for database connections
- **SSL/TLS**: Encrypted connections to all services
- **Secret Management**: Google Secret Manager for API keys

### Application Security
- **Rate Limiting**: 120 requests per minute per client
- **Input Validation**: Pydantic models for all API inputs
- **Container Scanning**: Automated vulnerability scanning in CI/CD

### Access Control
- **Service Accounts**: Least privilege access for each component
- **IAM Policies**: Fine-grained permissions for cloud resources
- **Audit Logging**: All database operations logged in PostgreSQL

## ğŸ“ˆ Monitoring & Observability

### Application Metrics
- **Agent Performance**: Response times, success rates, token usage
- **Database Health**: Connection pool status, query performance
- **Business Metrics**: Agent selection accuracy, user satisfaction

### Infrastructure Monitoring
- **OpenTelemetry**: Distributed tracing across all services
- **Structured Logging**: JSON logs with correlation IDs
- **Health Checks**: Automated monitoring of all endpoints

### Alerting
- **Error Rate**: Alert when error rate > 5%
- **Response Time**: Alert when p95 > 2 seconds
- **Resource Usage**: Alert when CPU/memory > 80%

## ğŸ› ï¸ Troubleshooting

### Common Issues

**Vector Database Connection Issues**:
```bash
# Check vector DB type configuration
echo $VECTOR_DB_TYPE

# Test database connectivity
curl https://your-api-url/smart/status
```

**Agent Selection Not Working**:
```bash
# Check orchestrator analysis
curl -X POST https://your-api-url/smart/analyze \
  -H "Content-Type: application/json" \
  -d '{"question": "test query"}'
```

**Performance Issues**:
```bash
# Check metrics endpoint
curl https://your-api-url/metrics

# Review agent performance stats
curl https://your-api-url/smart/agents/comparison
```

### Debugging Commands

```bash
# Check container logs
kubectl logs deployment/py-ai-api

# Database connection test
kubectl exec -it deployment/py-ai-api -- python -c "
from app.database.mongodb_adapter import get_mongodb_adapter
import asyncio
async def test(): 
    adapter = await get_mongodb_adapter()
    stats = await adapter.get_statistics()
    print(stats)
asyncio.run(test())
"

# Agent status check
kubectl exec -it deployment/py-ai-api -- python -c "
from app.smart_orchestrator import SmartOrchestrator
import asyncio
async def test():
    orch = SmartOrchestrator()
    decision = await orch.analyze_task('test query')
    print(decision.model_dump())
asyncio.run(test())
"
```

## ğŸ¯ Next Steps

1. **Monitor Performance**: Use deployment for 1-2 weeks to gather baseline metrics
2. **Optimize Agent Selection**: Fine-tune orchestrator rules based on usage patterns
3. **Scale Vector Database**: Expand to additional cloud providers as needed
4. **Implement Custom Plugins**: Build company-specific extensions
5. **Add More Agents**: Integrate additional agent architectures as requirements evolve

## ğŸ“š Additional Resources

- **Harness Documentation**: https://docs.harness.io/
- **Terraform GCP Provider**: https://registry.terraform.io/providers/hashicorp/google/
- **Vertex AI Documentation**: https://cloud.google.com/vertex-ai/docs
- **CockroachDB Vector**: https://www.cockroachlabs.com/docs/stable/vector-data
- **Snowflake Cortex**: https://docs.snowflake.com/en/user-guide/ml-powered-functions

---

**Deployment completed successfully!** ğŸš€

Your enterprise multi-agent platform is now running with:
- âœ… **4 Agent Architectures** operational
- âœ… **Multi-Cloud Vector Database** configured  
- âœ… **Enterprise CI/CD Pipeline** active
- âœ… **Production Monitoring** enabled
- âœ… **Security Scanning** integrated

Ready for production workloads! ğŸ¯